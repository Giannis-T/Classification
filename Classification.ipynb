{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ερωτηση 2\n",
    "### 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn.utils as utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "import sklearn.linear_model as linear_model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import gensim \n",
    "import gensim.models\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.utils as gutils \n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(951, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MUTTqe8uqyMdBl186RmNeA</td>\n",
       "      <td>Japanese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>QdN72BWoyFypdGJhhI5r7g</td>\n",
       "      <td>Italian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WYYdQDjx-DsCanlP0DpImQ</td>\n",
       "      <td>Japanese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O1oZpbZNDMH_gz8DhsZCdA</td>\n",
       "      <td>Burgers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dChRGpit9fM_kZK5pafNyA</td>\n",
       "      <td>Burgers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id  category\n",
       "0  MUTTqe8uqyMdBl186RmNeA  Japanese\n",
       "1  QdN72BWoyFypdGJhhI5r7g   Italian\n",
       "2  WYYdQDjx-DsCanlP0DpImQ  Japanese\n",
       "3  O1oZpbZNDMH_gz8DhsZCdA   Burgers\n",
       "4  dChRGpit9fM_kZK5pafNyA   Burgers"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bus = pd.read_csv(\"philly_restaurants_categories.csv\")\n",
    "print(df_bus.shape)\n",
    "df_bus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(153412, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8JFGBuHMoiNDyfcxuWNtrA</td>\n",
       "      <td>smOvOajNG0lS4Pq7d8g4JQ</td>\n",
       "      <td>RZtGWDLCAtuipwaZ-UfjmQ</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Good food--loved the gnocchi with marinara\\nth...</td>\n",
       "      <td>2009-10-14 19:57:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>cvQXRFLCyr0S7EgFb4lZqw</td>\n",
       "      <td>ZGjgfSvjQK886kiTzLwfLQ</td>\n",
       "      <td>EtKSTHV5Qx_Q7Aur9o4kQQ</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>On a scale of one to things that are awesome, ...</td>\n",
       "      <td>2009-10-14 01:15:04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 review_id                 user_id             business_id  \\\n",
       "3   8JFGBuHMoiNDyfcxuWNtrA  smOvOajNG0lS4Pq7d8g4JQ  RZtGWDLCAtuipwaZ-UfjmQ   \n",
       "10  cvQXRFLCyr0S7EgFb4lZqw  ZGjgfSvjQK886kiTzLwfLQ  EtKSTHV5Qx_Q7Aur9o4kQQ   \n",
       "\n",
       "    stars  useful  funny  cool  \\\n",
       "3     4.0       0      0     0   \n",
       "10    5.0       3      1     1   \n",
       "\n",
       "                                                 text                 date  \n",
       "3   Good food--loved the gnocchi with marinara\\nth...  2009-10-14 19:57:14  \n",
       "10  On a scale of one to things that are awesome, ...  2009-10-14 01:15:04  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rev = pd.read_csv(\"reviews.csv\")\n",
    "df_rev = df_rev[df_rev.business_id.isin(df_bus.business_id)]\n",
    "print(df_rev.shape)\n",
    "df_rev.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(951, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MUTTqe8uqyMdBl186RmNeA</td>\n",
       "      <td>Japanese</td>\n",
       "      <td>Stopped in to check out this new spot around t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>QdN72BWoyFypdGJhhI5r7g</td>\n",
       "      <td>Italian</td>\n",
       "      <td>This place is top notch, with phenomenal servi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WYYdQDjx-DsCanlP0DpImQ</td>\n",
       "      <td>Japanese</td>\n",
       "      <td>Delicious and very nicely presented. We had J ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O1oZpbZNDMH_gz8DhsZCdA</td>\n",
       "      <td>Burgers</td>\n",
       "      <td>This Wendy's is the worst Wendy's to go to . T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dChRGpit9fM_kZK5pafNyA</td>\n",
       "      <td>Burgers</td>\n",
       "      <td>I am not known for favorable restaurant review...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id  category  \\\n",
       "0  MUTTqe8uqyMdBl186RmNeA  Japanese   \n",
       "1  QdN72BWoyFypdGJhhI5r7g   Italian   \n",
       "2  WYYdQDjx-DsCanlP0DpImQ  Japanese   \n",
       "3  O1oZpbZNDMH_gz8DhsZCdA   Burgers   \n",
       "4  dChRGpit9fM_kZK5pafNyA   Burgers   \n",
       "\n",
       "                                                text  \n",
       "0  Stopped in to check out this new spot around t...  \n",
       "1  This place is top notch, with phenomenal servi...  \n",
       "2  Delicious and very nicely presented. We had J ...  \n",
       "3  This Wendy's is the worst Wendy's to go to . T...  \n",
       "4  I am not known for favorable restaurant review...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --------The sum of reviews for each business seperate\n",
    "df_rev = pd.read_csv(\"reviews.csv\")\n",
    "df_rev = df_rev[df_rev.business_id.isin(df_bus.business_id)]\n",
    "df_rev = df_rev.groupby(by= \"business_id\")[\"text\"].sum()\n",
    "reviews_by_business = df_bus.merge(df_rev, on=\"business_id\")\n",
    "print(reviews_by_business.shape)\n",
    "reviews_by_business.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Σε αυτό το σημείο ανακατεύουμε τα reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(951, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>kvad5dDPLZ-wuKJmMIs0Ag</td>\n",
       "      <td>Burgers</td>\n",
       "      <td>My friend, her baby, and myself came in on a S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>678</th>\n",
       "      <td>JtDZvV3HddxFnNdrwrWh7g</td>\n",
       "      <td>Italian</td>\n",
       "      <td>Actually, this is a 4.5 star review. My wife K...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>915</th>\n",
       "      <td>f_bUsM34FGpzECag3Cn8gw</td>\n",
       "      <td>Italian</td>\n",
       "      <td>The food I ordered was great the service howev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>TjwWB-ET-qmO2-8bfIHMig</td>\n",
       "      <td>Japanese</td>\n",
       "      <td>Had lunch here today and really, really enjoye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584</th>\n",
       "      <td>AqX_5srdyAS1tNiqC3mDvg</td>\n",
       "      <td>Italian</td>\n",
       "      <td>I was here for the first time yesterday for th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                business_id  category  \\\n",
       "629  kvad5dDPLZ-wuKJmMIs0Ag   Burgers   \n",
       "678  JtDZvV3HddxFnNdrwrWh7g   Italian   \n",
       "915  f_bUsM34FGpzECag3Cn8gw   Italian   \n",
       "129  TjwWB-ET-qmO2-8bfIHMig  Japanese   \n",
       "584  AqX_5srdyAS1tNiqC3mDvg   Italian   \n",
       "\n",
       "                                                  text  \n",
       "629  My friend, her baby, and myself came in on a S...  \n",
       "678  Actually, this is a 4.5 star review. My wife K...  \n",
       "915  The food I ordered was great the service howev...  \n",
       "129  Had lunch here today and really, really enjoye...  \n",
       "584  I was here for the first time yesterday for th...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SX = utils.shuffle(reviews_by_business) \n",
    "print(SX.shape)\n",
    "SX.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Στο παρακάτω block παίρνω 5 Training, Testing σύνολα από τα ανακατεμένα δεδομένα, όπου σε κάθε fold αντιστοιχώ τα indices που δόθηκαν από το kfold στις πραγματικές τιμές των δεδομένων. Εδώ  δημιουργώ ένα tf-idf vectorizer και παίρνω τις διανυσματικές τιμές των κειμένων των training και testing συνόλων. Αυτές τις τιμές τις χρησιμοποιώ για να κάνω training σε 3 classifiers. Στο τελευταίο fold, στο Logistic Regressor παίρνω τις 20 λέξεις με το μεγαλύτερο και μικρότερο βάρος που χρησιμοποιούνται για τη κατηγοριοποίηση. Από κάθε fold για κάθε αλγόριθμο αποθηκεύω τις μετρικές accuracy, confusion matrix, precision, recall, F1-measure για να υπολογίσω το μέσο αυτών των μετρικών στα 5 folds για κάθε αλγόριθμο ξεχωριστά."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: (760,) TEST: (191,)\n",
      "Completed Fold_1\n",
      "------------\n",
      "TRAIN: (761,) TEST: (190,)\n",
      "Completed Fold_2\n",
      "------------\n",
      "TRAIN: (761,) TEST: (190,)\n",
      "Completed Fold_3\n",
      "------------\n",
      "TRAIN: (761,) TEST: (190,)\n",
      "Completed Fold_4\n",
      "------------\n",
      "TRAIN: (761,) TEST: (190,)\n",
      "------------------------------------\n",
      "Top 10 words with the Highest weight in the 5th Fold with the Logistic Regressor Classifier:\n",
      "Category Burgers:\n",
      "burger fries sandwich wings line drink inside server special cheesesteak steak waitress actually seated prices awesome half bar eating beer \n",
      "Category Italian:\n",
      "italian pasta pizza wine bread dinner salad street night meat perfectly week atmosphere happy perfect excellent lunch disappointed selection highly \n",
      "Category Japanese:\n",
      "sushi ramen spicy fried shrimp sweet pork soup flavor roll rolls reservation options dishes salmon nwe drinks fish tasty different \n",
      "\n",
      "Top 10 words with the Lowest weight in the 5th Fold with the Logistic Regressor Classifier:\n",
      "Category Burgers:\n",
      "dessert reservation fried lunch shrimp salad dishes roll street pizza dinner flavor soup spicy wine pork ramen pasta sushi italian \n",
      "Category Italian:\n",
      "kind eating actually seated line fish options special inside shrimp sweet spicy fried salmon drink burger ramen wings fries sushi \n",
      "Category Japanese:\n",
      "awesome outside night city server beer bar dinner waitress line wine steak cheesesteak fries bread pasta italian burger sandwich pizza \n",
      "Completed Fold_5\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "X = SX.text\n",
    "y = SX.category\n",
    "\n",
    "accuracy_per_fold = []\n",
    "confusion_matrix_per_fold = []\n",
    "precision_per_fold = []\n",
    "recall_per_fold = []\n",
    "f1_measure_per_fold = []\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "for fold_num, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "\n",
    "    # Initializing the lists with the measurements for all algorithms in the current fold\n",
    "    accuracy_per_algo = []\n",
    "    confusion_matrix_per_algo = []\n",
    "    precision_per_algo = []\n",
    "    recall_per_algo = []\n",
    "    f1_measure_per_algo = []\n",
    "\n",
    "    # Coresponding the indeces to the actual data\n",
    "    print(\"TRAIN:\", train_index.shape, \"TEST:\", test_index.shape)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Getting the tf-idf representation of the training and testing data of the fold\n",
    "    vectorizer = TfidfVectorizer(stop_words = \"english\", max_features= 100, min_df=4,max_df=0.8)\n",
    "    X_train = vectorizer.fit_transform(X_train)\n",
    "    X_test = vectorizer.transform(X_test)\n",
    "\n",
    "    #-------SVM\n",
    "    svm_clf = svm.SVC()\n",
    "    svm_clf.fit(X_train,y_train)\n",
    "    y_pred = svm_clf.predict(X_test)\n",
    "\n",
    "    accuracy_per_algo.append(metrics.accuracy_score(y_test, y_pred))\n",
    "    confusion_matrix_per_algo.append(metrics.confusion_matrix(y_test, y_pred))\n",
    "    precision_per_algo.append(metrics.precision_score(y_test, y_pred, average=None))\n",
    "    recall_per_algo.append(metrics.recall_score(y_test, y_pred, average=None))\n",
    "    f1_measure_per_algo.append(metrics.f1_score(y_test,y_pred, average=None))\n",
    "\n",
    "    #-------K-NN\n",
    "    knn = KNeighborsClassifier()\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "\n",
    "    accuracy_per_algo.append(metrics.accuracy_score(y_test, y_pred))\n",
    "    confusion_matrix_per_algo.append(metrics.confusion_matrix(y_test, y_pred))\n",
    "    precision_per_algo.append(metrics.precision_score(y_test, y_pred, average=None))\n",
    "    recall_per_algo.append(metrics.recall_score(y_test, y_pred, average=None))\n",
    "    f1_measure_per_algo.append(metrics.f1_score(y_test,y_pred, average=None))\n",
    "\n",
    "#------Logistic Regression\n",
    "    lr_clf = linear_model.LogisticRegression(solver='lbfgs')\n",
    "    lr_clf.fit(X_train, y_train)\n",
    "    y_pred = lr_clf.predict(X_test)\n",
    "    if fold_num == 4:\n",
    "        categories = lr_clf.coef_.argsort()\n",
    "        categories = categories[:,::-1]\n",
    "        terms = vectorizer.get_feature_names()\n",
    "\n",
    "        print(\"------------------------------------\\nTop 10 words with the Highest weight in the 5th Fold with the Logistic Regressor Classifier:\")\n",
    "        for i in range(len(categories)):\n",
    "            print(f\"Category {lr_clf.classes_[i]}:\")\n",
    "            str_words = \"\"\n",
    "            for j in categories[i, :20]:\n",
    "                str_words += terms[j] + \" \"\n",
    "            print(str_words)\n",
    "\n",
    "        print(\"\\nTop 10 words with the Lowest weight in the 5th Fold with the Logistic Regressor Classifier:\")\n",
    "        for i in range(len(categories)):\n",
    "            print(f\"Category {lr_clf.classes_[i]}:\")\n",
    "            str_words = \"\"\n",
    "            for j in categories[i, -20:]:\n",
    "                str_words += terms[j] + \" \"\n",
    "            print(str_words)\n",
    "   \n",
    "    accuracy_per_algo.append(metrics.accuracy_score(y_test, y_pred))\n",
    "    confusion_matrix_per_algo.append(metrics.confusion_matrix(y_test, y_pred))\n",
    "    precision_per_algo.append(metrics.precision_score(y_test, y_pred, average=None))\n",
    "    recall_per_algo.append(metrics.recall_score(y_test, y_pred, average=None))\n",
    "    f1_measure_per_algo.append(metrics.f1_score(y_test,y_pred, average=None))\n",
    "\n",
    "\n",
    "    # Saving the measurements from the current fold \n",
    "    accuracy_per_fold.append(accuracy_per_algo)\n",
    "    confusion_matrix_per_fold.append(confusion_matrix_per_algo)\n",
    "    precision_per_fold.append(precision_per_algo)\n",
    "    recall_per_fold.append(recall_per_algo)\n",
    "    f1_measure_per_fold.append(f1_measure_per_algo)\n",
    "    print(f\"Completed Fold_{fold_num+1}\\n------------\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Οι λέξεις με το μεγαλύτερο βάρος είναι αυτές που καθορίζουν την κατηγορία και έχουν το μεγαλύτερο βάρος στη κατηγορία που ανήκουν. Λέξεις όπως burger , fries καθορίζουν την κατηγορία Burger; italian, pizza, pasta την κατηγορία Italian και sushi, ramen, rice για την Japanese.<br><br>\n",
    "\n",
    "Βλέπουμε ότι οι λέξεις με το μικρότερο βάρος σε κάθε κατηγορία είναι κοινές με αυτές άλλων κατηγοριών, κάνοντας τη κατηγοριοποίηση πιο δύσκολη, για αυτό και έχουν το μικρότερο βάρος."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Στο παρακάτω block υπολογίζω τα average measurements σε 5 fold, για κάθε αλγόριθμο σε κάθε κλάση. Συγκεκριμένα, έχουμε τα measurements που είναι για κάθε fold(fold->algo), ενώ εμείς θέλουμε να είναι για κάθε αλγόριθμο algorithm ->fold. \n",
    "<br>Για να το πέτυχω αυτό χρησιμοποιώ ένα απλό αλγόριθμο που βρήκα στο stack-overfow και έχω ελέγξει ότι δουλεύει."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-----SVM------\n",
      "Accuracy: 0.9179939377238908\n",
      "Average confusion matrix:\n",
      " [[55.8  7.2  0.2]\n",
      " [ 5.4 80.2  0.6]\n",
      " [ 2.   0.2 38.6]]\n",
      "            ['Burgers' 'Italian' 'Japanese']\n",
      "Precision: [0.88372235 0.91552093 0.97876192]\n",
      "Recall: [0.88369477 0.9300495  0.9455732 ]\n",
      "F1-measure: [0.88328738 0.92227254 0.96153732]\n",
      "\n",
      "\n",
      "-----K-NN------\n",
      "Accuracy: 0.8874951777349132\n",
      "Average confusion matrix:\n",
      " [[52.2 10.4  0.6]\n",
      " [ 6.  79.4  0.8]\n",
      " [ 2.4  1.2 37.2]]\n",
      "            ['Burgers' 'Italian' 'Japanese']\n",
      "Precision: [0.86200539 0.87253116 0.96238599]\n",
      "Recall: [0.8252643  0.9205566  0.91120176]\n",
      "F1-measure: [0.84283867 0.89562886 0.9357225 ]\n",
      "\n",
      "\n",
      "-----Logistic Regression------\n",
      "Accuracy: 0.9232350509782309\n",
      "Average confusion matrix:\n",
      " [[56.6  6.4  0.2]\n",
      " [ 4.8 80.8  0.6]\n",
      " [ 2.2  0.4 38.2]]\n",
      "            ['Burgers' 'Italian' 'Japanese']\n",
      "Precision: [0.89052615 0.92302459 0.98017136]\n",
      "Recall: [0.89694362 0.93742817 0.93363805]\n",
      "F1-measure: [0.89317778 0.92960642 0.95550701]\n"
     ]
    }
   ],
   "source": [
    "algo_list = [\"SVM\", \"K-NN\", \"Logistic Regression\"]\n",
    "\n",
    "accuracy_per_algo_fold = list(map(list, zip(*accuracy_per_fold)))\n",
    "accuracy_per_algo_fold = np.array(accuracy_per_algo_fold)\n",
    "\n",
    "confusion_matrix_per_algo_fold = list(map(list, zip(*confusion_matrix_per_fold)))\n",
    "confusion_matrix_per_algo_fold = np.array(confusion_matrix_per_algo_fold)\n",
    "\n",
    "precision_per_algo_fold = list(map(list, zip(*precision_per_fold)))\n",
    "precision_per_algo_fold = np.array(precision_per_algo_fold)\n",
    "\n",
    "recall_per_algo_fold = list(map(list, zip(*recall_per_fold)))\n",
    "recall_per_algo_fold = np.array(recall_per_algo_fold)\n",
    "\n",
    "f1_measure_per_algo_fold = list(map(list, zip(*f1_measure_per_fold)))\n",
    "f1_measure_per_algo_fold = np.array(f1_measure_per_algo_fold)\n",
    "\n",
    "\n",
    "for i, algo in enumerate(algo_list):\n",
    "    print(f\"\\n\\n-----{algo}------\")\n",
    "    print(\"Accuracy:\", np.mean(accuracy_per_algo_fold[i]))\n",
    "    print(\"Average confusion matrix:\\n\", np.mean(confusion_matrix_per_algo_fold[i], axis=0))\n",
    "    print(f\"            {lr_clf.classes_}\")\n",
    "    print(\"Precision:\", np.mean(precision_per_algo_fold[i].T, axis=1))\n",
    "    print(\"Recall:\", np.mean(recall_per_algo_fold[i].T, axis=1))\n",
    "    print(\"F1-measure:\", np.mean(f1_measure_per_algo_fold[i].T, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.\n",
    "Το παρακάτω Block κάνει το pre-processing που χρειάζεται ώστε τα δεδομένα να έχουν την κατάλληλη μορφή για να χρησιμοποιηθούν από τα embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "951 951\n"
     ]
    }
   ],
   "source": [
    "train_gsim = [gutils.simple_preprocess(x) for x in SX.text]\n",
    "train_data_labels = [(x,y) for (x,y) in zip(train_gsim, SX.category) if len(x) > 0]\n",
    "\n",
    "X_gsim = [x for (x,y) in train_data_labels]\n",
    "y_gsim = [y for (x,y) in train_data_labels]\n",
    "print(len(X_gsim), len(y_gsim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: (760,) TEST: (191,)\n",
      "Finished CBOW Training\n",
      "Finished Skip-Gram Training\n",
      "Finished Doc2Vec Training\n",
      "Completed Fold_1\n",
      "------------\n",
      "TRAIN: (761,) TEST: (190,)\n",
      "Finished CBOW Training\n",
      "Finished Skip-Gram Training\n",
      "Finished Doc2Vec Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Giannis\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed Fold_2\n",
      "------------\n",
      "TRAIN: (761,) TEST: (190,)\n",
      "Finished CBOW Training\n",
      "Finished Skip-Gram Training\n",
      "Finished Doc2Vec Training\n",
      "Completed Fold_3\n",
      "------------\n",
      "TRAIN: (761,) TEST: (190,)\n",
      "Finished CBOW Training\n",
      "Finished Skip-Gram Training\n",
      "Finished Doc2Vec Training\n",
      "Completed Fold_4\n",
      "------------\n",
      "TRAIN: (761,) TEST: (190,)\n",
      "Finished CBOW Training\n",
      "Finished Skip-Gram Training\n",
      "Finished Doc2Vec Training\n",
      "Completed Fold_5\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "accuracy_per_fold = []\n",
    "accuracy_per_fold = []\n",
    "confusion_matrix_per_fold = []\n",
    "precision_per_fold = []\n",
    "recall_per_fold = []\n",
    "f1_measure_per_fold = []\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "for fold_num, (train_index, test_index) in enumerate(kf.split(X_gsim, y_gsim)):\n",
    "\n",
    "    # Initializing the lists with the measurements for all algorithms in the current fold\n",
    "    accuracy_per_emb = []\n",
    "    confusion_matrix_per_emb = []\n",
    "    precision_per_emb = []\n",
    "    recall_per_emb = []\n",
    "    f1_measure_per_emb = []\n",
    "\n",
    "    # Coresponding the indeces to the actual data in a more manual way than before.\n",
    "    print(\"TRAIN:\", train_index.shape, \"TEST:\", test_index.shape)\n",
    " \n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    for i in train_index:\n",
    "        X_train.append(X_gsim[i])\n",
    "        y_train.append(y_gsim[i])\n",
    "    for i in test_index:\n",
    "        X_test.append(X_gsim[i])\n",
    "        y_test.append(y_gsim[i])\n",
    "\n",
    "    # Training the CBOW model with the mean values of the word embedings\n",
    "    cbow_model = gensim.models.Word2Vec(X_train, vector_size = 50, sample=1e-05, min_count = 1, window = 10) \n",
    "    X_train_cbow = [np.array([cbow_model.wv[x] for x in y]).mean(axis = 0) for y in X_train]\n",
    "    X_test_cbow = [np.array([cbow_model.wv[x] for x in y if x in cbow_model.wv]).mean(axis = 0) for y in X_test]\n",
    "    print(\"Finished CBOW Training\")\n",
    "\n",
    "    # Training the  Skip-Gram model with the mean values of the word embedings\n",
    "    skipgram_model = gensim.models.Word2Vec(X_train, vector_size = 50, sample=1e-05, min_count = 1, window = 10, sg=1) \n",
    "    X_train_skipgram = [np.array([skipgram_model.wv[x] for x in y]).mean(axis = 0) for y in X_train]\n",
    "    X_test_skipgram = [np.array([skipgram_model.wv[x] for x in y if x in skipgram_model.wv]).mean(axis = 0) for y in X_test]\n",
    "    print(\"Finished Skip-Gram Training\")\n",
    "\n",
    "    # Training the Doc2Vec model\n",
    "    train_corpus = [gensim.models.doc2vec.TaggedDocument(X_train[i], [i]) for i in range(len(X_train))]\n",
    "    d2v_model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=10)\n",
    "    d2v_model.build_vocab(train_corpus)\n",
    "    d2v_model.train(train_corpus, total_examples=d2v_model.corpus_count, epochs=d2v_model.epochs)\n",
    "    X_train_d2v = [d2v_model.infer_vector(x) for x in X_train]\n",
    "    X_test_d2v = [d2v_model.infer_vector(x) for x in X_test]\n",
    "    print(\"Finished Doc2Vec Training\")\n",
    "    \n",
    "\n",
    "    for X_train, X_test in [(X_train_cbow, X_test_cbow), (X_train_skipgram, X_test_skipgram), (X_train_d2v, X_test_d2v)]:\n",
    "        \n",
    "        accuracy_per_algo = []\n",
    "        confusion_matrix_per_algo = []\n",
    "        precision_per_algo = []\n",
    "        recall_per_algo = []\n",
    "        f1_measure_per_algo = []\n",
    "        # SVM\n",
    "        svm_clf = svm.SVC()\n",
    "        svm_clf.fit(np.array(X_train), np.array(y_train))\n",
    "        y_pred = svm_clf.predict(np.array(X_test))\n",
    "\n",
    "        # Saving the results from the current algorithm\n",
    "        accuracy_per_algo.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        confusion_matrix_per_algo.append(metrics.confusion_matrix(y_test, y_pred))\n",
    "        precision_per_algo.append(metrics.precision_score(y_test, y_pred, average=None))\n",
    "        recall_per_algo.append(metrics.recall_score(y_test, y_pred, average=None))\n",
    "        f1_measure_per_algo.append(metrics.f1_score(y_test,y_pred, average=None))\n",
    "\n",
    "        # K-NN\n",
    "        knn = KNeighborsClassifier()\n",
    "        knn.fit(np.array(X_train), np.array(y_train))\n",
    "        y_pred = knn.predict(np.array(X_test))\n",
    "\n",
    "        # Saving the results from the current algorithm\n",
    "        accuracy_per_algo.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        confusion_matrix_per_algo.append(metrics.confusion_matrix(y_test, y_pred))\n",
    "        precision_per_algo.append(metrics.precision_score(y_test, y_pred, average=None))\n",
    "        recall_per_algo.append(metrics.recall_score(y_test, y_pred, average=None))\n",
    "        f1_measure_per_algo.append(metrics.f1_score(y_test,y_pred, average=None))\n",
    "        \n",
    "        # Logistic Regressor\n",
    "        lr_clf = linear_model.LogisticRegression(solver='lbfgs', max_iter=300)\n",
    "        lr_clf.fit(np.array(X_train), np.array(y_train))\n",
    "        y_pred = lr_clf.predict(np.array(X_test))\n",
    "\n",
    "        # Saving the results from the current algorithm\n",
    "        accuracy_per_algo.append(metrics.accuracy_score(y_test, y_pred))\n",
    "        confusion_matrix_per_algo.append(metrics.confusion_matrix(y_test, y_pred))\n",
    "        precision_per_algo.append(metrics.precision_score(y_test, y_pred, average=None))\n",
    "        recall_per_algo.append(metrics.recall_score(y_test, y_pred, average=None))\n",
    "        f1_measure_per_algo.append(metrics.f1_score(y_test,y_pred, average=None))\n",
    "\n",
    "        # Saving the measurements from the current embedding\n",
    "        accuracy_per_emb.append(accuracy_per_algo)\n",
    "        confusion_matrix_per_emb.append(confusion_matrix_per_algo)\n",
    "        precision_per_emb.append(precision_per_algo)\n",
    "        recall_per_emb.append(recall_per_algo)\n",
    "        f1_measure_per_emb.append(f1_measure_per_algo)\n",
    "        \n",
    "    # Saving the measurements from the current fold \n",
    "    accuracy_per_fold.append(accuracy_per_emb)\n",
    "    confusion_matrix_per_fold.append(confusion_matrix_per_emb)\n",
    "    precision_per_fold.append(precision_per_emb)\n",
    "    recall_per_fold.append(recall_per_emb)\n",
    "    f1_measure_per_fold.append(f1_measure_per_emb)\n",
    "    print(f\"Completed Fold_{fold_num+1}\\n------------\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Σε αυτό το σημείο για κάθε fold, για κάθε embedding, για κάθε αλγόριθμο έχουμε τις ιδίες μετρικές με το προηγούμενο ερώτημα στη μορφή fold->embedding->algorithm, ενώ εμείς τις θέλουμε στη μορφή algorithm->embedding->fold, για να υπολογίσουμε τις μέσες τιμές κάθε μετρικής για κάθε αλγόριθμο εκπαιδευμένος σε κάθε embedding στα 5 folds. Για να τα φέρουμε σε αυτή τη μορφή(algorithm->embedding->fold) θα πρέπει να εφαρμόσουμε τον αλγόριθμο που χρησιμοποιήσαμε στο προηγούμενο ερώτημα αλλά δυο φορές."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "~~~~~~~~ CBOW ~~~~~~~~\n",
      "\n",
      "----- SVM ------\n",
      "Accuracy: 0.8632956737393223\n",
      "Average confusion matrix:\n",
      " [[51.2 11.8  0.2]\n",
      " [ 9.6 76.2  0.4]\n",
      " [ 3.6  0.4 36.8]]\n",
      "            ['Burgers' 'Italian' 'Japanese']\n",
      "Precision: [0.7938974  0.85957322 0.98468835]\n",
      "Recall: [0.81347217 0.88297202 0.90080708]\n",
      "F1-measure: [0.80116724 0.87055587 0.93936379]\n",
      "\n",
      "----- K-NN ------\n",
      "Accuracy: 0.8748525764673463\n",
      "Average confusion matrix:\n",
      " [[53.  10.2  0. ]\n",
      " [ 9.8 75.8  0.6]\n",
      " [ 2.4  0.8 37.6]]\n",
      "            ['Burgers' 'Italian' 'Japanese']\n",
      "Precision: [0.8105293  0.87118123 0.98447368]\n",
      "Recall: [0.84149088 0.87843756 0.92117344]\n",
      "F1-measure: [0.82404964 0.87457111 0.9504946 ]\n",
      "\n",
      "----- Logistic Regression ------\n",
      "Accuracy: 0.8811628547809314\n",
      "Average confusion matrix:\n",
      " [[53.  10.   0.2]\n",
      " [ 9.2 76.4  0.6]\n",
      " [ 1.4  1.2 38.2]]\n",
      "            ['Burgers' 'Italian' 'Japanese']\n",
      "Precision: [0.83161825 0.87218456 0.98080808]\n",
      "Recall: [0.8389141  0.88541532 0.93573102]\n",
      "F1-measure: [0.83429921 0.87837168 0.95711688]\n",
      "\n",
      "~~~~~~~~ Skip-Gram ~~~~~~~~\n",
      "\n",
      "----- SVM ------\n",
      "Accuracy: 0.8811463213006338\n",
      "Average confusion matrix:\n",
      " [[47.8 15.4  0. ]\n",
      " [ 3.4 82.4  0.4]\n",
      " [ 1.   2.4 37.4]]\n",
      "            ['Burgers' 'Italian' 'Japanese']\n",
      "Precision: [0.91385153 0.82108812 0.98968254]\n",
      "Recall: [0.75904579 0.95542502 0.91551481]\n",
      "F1-measure: [0.82791611 0.88268945 0.94993759]\n",
      "\n",
      "----- K-NN ------\n",
      "Accuracy: 0.8927252686690549\n",
      "Average confusion matrix:\n",
      " [[52.8 10.   0.4]\n",
      " [ 6.8 78.6  0.8]\n",
      " [ 1.2  1.2 38.4]]\n",
      "            ['Burgers' 'Italian' 'Japanese']\n",
      "Precision: [0.8673687  0.87332107 0.9706619 ]\n",
      "Recall: [0.84087226 0.91008983 0.94161408]\n",
      "F1-measure: [0.85213459 0.89106197 0.95421389]\n",
      "\n",
      "----- Logistic Regression ------\n",
      "Accuracy: 0.8643207495177734\n",
      "Average confusion matrix:\n",
      " [[48.2 15.   0. ]\n",
      " [ 6.4 79.4  0.4]\n",
      " [ 1.   3.  36.8]]\n",
      "            ['Burgers' 'Italian' 'Japanese']\n",
      "Precision: [0.86422829 0.81373723 0.98959459]\n",
      "Recall: [0.76436641 0.9216761  0.90376804]\n",
      "F1-measure: [0.80998344 0.86394057 0.94333142]\n",
      "\n",
      "~~~~~~~~ Doc2Vec ~~~~~~~~\n",
      "\n",
      "----- SVM ------\n",
      "Accuracy: 0.9253127583356295\n",
      "Average confusion matrix:\n",
      " [[56.4  6.6  0.2]\n",
      " [ 5.8 80.   0.4]\n",
      " [ 0.6  0.6 39.6]]\n",
      "            ['Burgers' 'Italian' 'Japanese']\n",
      "Precision: [0.89913735 0.91499282 0.98529227]\n",
      "Recall: [0.89550936 0.92587738 0.97329227]\n",
      "F1-measure: [0.89662749 0.92021277 0.97903266]\n",
      "\n",
      "----- K-NN ------\n",
      "Accuracy: 0.9085202535133646\n",
      "Average confusion matrix:\n",
      " [[55.4  7.8  0. ]\n",
      " [ 7.8 78.   0.4]\n",
      " [ 0.8  0.6 39.4]]\n",
      "            ['Burgers' 'Italian' 'Japanese']\n",
      "Precision: [0.86541308 0.90073652 0.98968254]\n",
      "Recall: [0.88123877 0.90260975 0.96506815]\n",
      "F1-measure: [0.87265957 0.90142367 0.9771432 ]\n",
      "\n",
      "----- Logistic Regression ------\n",
      "Accuracy: 0.9053458252962248\n",
      "Average confusion matrix:\n",
      " [[55.2  7.4  0.6]\n",
      " [ 7.4 78.2  0.6]\n",
      " [ 1.6  0.4 38.8]]\n",
      "            ['Burgers' 'Italian' 'Japanese']\n",
      "Precision: [0.85874872 0.90749147 0.97140338]\n",
      "Recall: [0.87459899 0.9058526  0.9532104 ]\n",
      "F1-measure: [0.86593407 0.90639004 0.96215749]\n"
     ]
    }
   ],
   "source": [
    "algo_list = [\"SVM\", \"K-NN\", \"Logistic Regression\"]\n",
    "embedding_list = [\"CBOW\", \"Skip-Gram\", \"Doc2Vec\"]\n",
    "\n",
    "accuracy_per_algo_fold = list(map(list, zip(*accuracy_per_fold)))\n",
    "accuracy_per_algo_fold = np.array(accuracy_per_algo_fold)\n",
    "\n",
    "confusion_matrix_per_algo_fold = list(map(list, zip(*confusion_matrix_per_fold)))\n",
    "confusion_matrix_per_algo_fold = np.array(confusion_matrix_per_algo_fold)\n",
    "\n",
    "precision_per_algo_fold = list(map(list, zip(*precision_per_fold)))\n",
    "precision_per_algo_fold = np.array(precision_per_algo_fold)\n",
    "\n",
    "recall_per_algo_fold = list(map(list, zip(*recall_per_fold)))\n",
    "recall_per_algo_fold = np.array(recall_per_algo_fold)\n",
    "\n",
    "f1_measure_per_algo_fold = list(map(list, zip(*f1_measure_per_fold)))\n",
    "f1_measure_per_algo_fold = np.array(f1_measure_per_algo_fold)\n",
    "\n",
    "accuracy_per_emb_algo_fold = []\n",
    "confusion_matrix_per_emb_algo_fold = []\n",
    "precision_per_emb_algo_fold = []\n",
    "recall_per_emb_algo_fold = []\n",
    "f1_measure_per_emb_algo_fold = []\n",
    "\n",
    "for i in range(len(embedding_list)):\n",
    "    list2 = list(map(list, zip(*accuracy_per_algo_fold[i]))) \n",
    "    list2= np.array(list2)\n",
    "    accuracy_per_emb_algo_fold.append(list2)\n",
    "\n",
    "    list2 = list(map(list, zip(*confusion_matrix_per_algo_fold[i]))) \n",
    "    list2= np.array(list2)\n",
    "    confusion_matrix_per_emb_algo_fold.append(list2)\n",
    "\n",
    "    list2 = list(map(list, zip(*precision_per_algo_fold[i]))) \n",
    "    list2= np.array(list2)\n",
    "    precision_per_emb_algo_fold.append(list2)\n",
    "\n",
    "    list2 = list(map(list, zip(*recall_per_algo_fold[i]))) \n",
    "    list2= np.array(list2)\n",
    "    recall_per_emb_algo_fold.append(list2)\n",
    "\n",
    "    list2 = list(map(list, zip(*f1_measure_per_algo_fold[i]))) \n",
    "    list2= np.array(list2)\n",
    "    f1_measure_per_emb_algo_fold.append(list2)\n",
    "    \n",
    "\n",
    "for emb_index, emb in enumerate(embedding_list):\n",
    "    print(f\"\\n~~~~~~~~ {emb} ~~~~~~~~\")\n",
    "    for i, algo in enumerate(algo_list):\n",
    "        print(f\"\\n----- {algo} ------\")\n",
    "        print(\"Accuracy:\", np.mean(accuracy_per_emb_algo_fold[emb_index][i]))\n",
    "        print(\"Average confusion matrix:\\n\", np.mean(confusion_matrix_per_emb_algo_fold[emb_index][i], axis=0))\n",
    "        print(f\"            {lr_clf.classes_}\")\n",
    "        print(\"Precision:\", np.mean(precision_per_emb_algo_fold[emb_index][i].T, axis=1))\n",
    "        print(\"Recall:\", np.mean(recall_per_emb_algo_fold[emb_index][i].T, axis=1))\n",
    "        print(\"F1-measure:\", np.mean(f1_measure_per_emb_algo_fold[emb_index][i].T, axis=1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# path = api.load(\"word2vec-google-news-300\", return_path=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g_model = gensim.models.KeyedVectors.load_word2vec_format(path, binary=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train_gmodel = [[g_model[x] for x in y if x in g_model] for y in X_gsim[:800]]\n",
    "# train_data_labels = [(x,y) for (x,y) in zip(train_gmodel, SX.category[:800]) if len(x) > 0]\n",
    "# X_train= [np.array(x) for (x,y) in train_data_labels]\n",
    "# y_train = [y for (x,y) in train_data_labels]\n",
    "\n",
    "# X_train = [x.mean(axis = 0) for x in X_train]\n",
    "\n",
    "\n",
    "# test_gmodel = [[g_model[x] for x in y if x in g_model] for y in X_gsim[800:]]\n",
    "# test_data_labels = [(x,y) for (x,y) in zip(test_gmodel, SX.category[800:]) if len(x) > 0]\n",
    "# X_test = [np.array(x) for (x,y) in test_data_labels]\n",
    "# y_test = [y for (x,y) in test_data_labels]\n",
    "\n",
    "# X_test = [x.mean(axis = 0) for x in X_test]\n",
    "\n",
    "# lr_clf = linear_model.LogisticRegression(solver='lbfgs', max_iter=300)\n",
    "# lr_clf.fit(X_train, np.array(y_train))\n",
    "# lr_clf.score(np.array(X_test),y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "5bf5137ea94996cf045b4e01cc05ea6528fbc1ec529e737db76b4d8a5ada53af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
